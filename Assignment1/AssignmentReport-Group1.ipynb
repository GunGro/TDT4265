{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "We will show that the element $(n, 1, i)$ of the gradient matrix array has the following form \n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_i} = -(y^n - \\hat{y}^n) x_i^n\n",
    "\\end{equation*}\n",
    "\n",
    "We start with the chain rule.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_i} = \\dfrac{\\partial C^n}{\\partial sum}\\dfrac{\\partial sum}{\\partial \\omega_i}\n",
    "\\end{equation*}\n",
    "Furthermore, the sum is given as\n",
    "\\begin{equation*}\n",
    "sum = \\sum_{i = 1}^m x_i\\omega_i = x^T\\omega\n",
    "\\end{equation*}\n",
    "\n",
    "which means we can say\n",
    "\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_i} = \\dfrac{\\partial C^n}{\\partial sum} x_i^n.\n",
    "\\end{equation*}\n",
    "\n",
    "Then we apply the activation function to the incomming sum of weighted inputs, $sum$. We use the chain rule again. The activation function is given by the sigmoid(special case of the logistic family) function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma(x) = \\dfrac{1}{1+ \\exp(-x)},\n",
    "\\end{equation*}\n",
    "with derivative\n",
    "\\begin{equation*}\n",
    "\\partial_x\\sigma(x) = \\sigma(x)\\left(1-\\sigma(x)\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Thus, the gradient simplifies to\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_i} = \\dfrac{\\partial C^n}{\\partial \\hat{y}^n} \\dfrac{\\partial \\hat{y}^n}{\\partial sum^n} x_i^n = \\dfrac{\\partial C^n}{\\partial \\hat{y}^n} \\hat{y}^n(1-\\hat{y}^n) x_i^n. \n",
    "\\end{equation*}\n",
    "\n",
    "Lastly, the cost function, given by \n",
    "\n",
    "\\begin{equation*}\n",
    "C(\\hat{y}, y) = -y\\ln(\\hat{y}) - (1-y)ln(1-\\hat{y}) \n",
    "\\end{equation*}\n",
    "with derivative\n",
    "\\begin{equation*}\n",
    "\\partial_{\\hat{y}}C(\\hat{y}, y) = -\\dfrac{y}{\\hat{y}} + \\dfrac{1-y}{1-\\hat{y}} \n",
    "\\end{equation*}\n",
    "\n",
    "which simplifies the expression of the gradient\n",
    "\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n(\\omega)}{\\partial \\omega_i} = \\left(-\\dfrac{y^n}{\\hat{y}^n} + \\dfrac{1-y^n}{1-\\hat{y}^n}\\right)  \\hat{y}^n(1-\\hat{y}^n) x_i^n = \\left(-y^n(1-\\hat{y}^n) + \\hat{y}^n(1-y^n)\\right)x_i^n = -(y^n - \\hat{y}^n ) x_i^n. \n",
    "\\end{equation*}\n",
    "\n",
    "q.e.d.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "We will now look at the situation where one layer, denoted $x$, of size $r$ upstream passes its weighted sum of inputs to a layer, denoted $y$, of size $m$. The activation function of layer $y$ is the identity(linear) function. The output of layer $y$ is then passed onto a layer $\\hat{z}$, a softmax-layer. The outputs of layer $\\hat{z}$ is compared to hard one-hot encoded target variables through the multivariate cross-entropy loss function. It is an often used convension of machine learning not to refer to the softmax as an activation function, but rather a specific type of layer( other examples of layer types are: normalizing layer, and drop-out layer). Lets look at the soft-max transformation. It is a vector evaluated function with elements\n",
    "\\begin{equation*}\n",
    "S_i(x) =  \\dfrac{\\exp(x_i)}{\\sum\\limits_{j= 1}^m \\exp(x_j)}.\n",
    "\\end{equation*}\n",
    "The Jacobian of the softmax-transformation can be split into two cases. The off-diagonal and main diagonal elements. The main diagonal elements are\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial S_i(x)}{\\partial x_i} =  \\dfrac{\\exp(x_i)\\left(\\sum\\limits_{j= 1}^m \\exp(x_j)\\right) - \\exp(x_i)^2}{\\left(\\sum\\limits_{j= 1}^m \\exp(x_j)\\right)^2} = S_i(x)-S_i(x)^2 = S_i(x)\\left(1-S_i(x)\\right).\n",
    "\\end{equation*}\n",
    "While the off-diagonal elements, $k\\neq i$, are \n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial S_i(x)}{\\partial x_k} =  \\dfrac{- \\exp(x_i)\\exp(x_k)}{\\left(\\sum\\limits_{j= 1}^m \\exp(x_j)\\right)^2} = -S_i(x)S_k(x).\n",
    "\\end{equation*}\n",
    "\n",
    "We look at the multivariate cross-entropy loss function, given by the sum\n",
    "\\begin{equation*}\n",
    "C(p, t) = \\sum\\limits_{i= 1}^m C_i(p, t) = \\sum\\limits_{i= 1}^m -t_i \\ln p_i,\n",
    "\\end{equation*}\n",
    "where $t$ is a one-hot-encoded target vector, and $p$ is the probability estimate output-vector. The Jacobian is a diagonal matrix \n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C(\\hat{z}, z)}{\\partial \\hat{z}} = \\mathrm{Diag}\\left(-\\dfrac{z}{\\hat{z}}\\right),\n",
    "\\end{equation*}\n",
    "where ${z}\\,/\\,{\\hat{z}}$ is vectorized element-wise division. \n",
    "\n",
    "We start the calculation of the gradient tensor, without using matrix notation and with a batch index number $n$. **OBS**: we will denote $\\omega_{kj}$ as the weight connecting node $k$ upstream to node $j$ downstream. This yields \n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_{kj}} = \\dfrac{\\partial C^n}{\\partial sum_j^n}\\dfrac{\\partial sum_j^n}{\\partial \\omega_{kj}} = \\dfrac{\\partial C_i^n}{\\partial y_j^n}\\dfrac{\\partial y_j^n}{\\partial sum_j^n}\\dfrac{\\partial sum_j^n}{\\partial \\omega_{kj}} =  \\dfrac{\\partial C_i^n}{\\partial y_j^n}x^n_k,\n",
    "\\end{equation*}\n",
    "where $\\dfrac{\\partial y_j^n}{\\partial sum_j^n} = 1$ is the derivative of the identity activation function, and $\\dfrac{\\partial sum_j^n}{\\partial \\omega_{kj}} = x^n_k$.\n",
    "Applying the chain rule again, over the soft-max layer, we get\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_{kj}} = x^n_k \\dfrac{\\partial C^n}{\\partial y_j^n} = x^n_k \\sum\\limits_{l = 1}^m \\dfrac{\\partial C^n}{\\partial \\hat{z}_l^n} \\dfrac{\\partial \\hat{z}_l^n}{\\partial y_j^n} =  - x^n_k \\sum\\limits_{l = 1}^m \\dfrac{z_l^n}{ \\hat{z}_l^n} \\dfrac{\\partial \\hat{z}_l^n}{\\partial y_j^n}.\n",
    "\\end{equation*}\n",
    "\n",
    "Furthermore, we seperate the case with $l = j$ from the rest, giving \n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_{kj}} = - x^n_k \\sum\\limits_{l = 1}^m \\dfrac{z_l^n}{ \\hat{z}_l^n} \\dfrac{\\partial \\hat{z}_l^n}{\\partial y_j^n} =   - x^n_k \\left(\\dfrac{z_j^n}{ \\hat{z}_j^n} \\dfrac{\\partial \\hat{z}_j^n}{\\partial y_j^n}+\\sum\\limits_{l = 1. l\\neq j}^m \\dfrac{z_l^n}{ \\hat{z}_l^n} \\dfrac{\\partial \\hat{z}_l^n}{\\partial y_j^n}\\right) = - x^n_k \\left(\\dfrac{z_j^n}{ \\hat{z}_j^n} \\hat{z}_j^n(1-\\hat{z}_j^n)-\\sum\\limits_{l = 1. l\\neq j}^m \\dfrac{z_l^n}{ \\hat{z}_l^n} \\hat{z}_j^n\\hat{z}_l^n\\right) = - x^n_k \\left(z_j^n(1-\\hat{z}_j^n)-\\hat{z}_j^n\\sum\\limits_{l = 1. l\\neq j}^m z_l^n \\right).\n",
    "\\end{equation*}\n",
    "\n",
    "Since the vector $z^n$ is one-hot-encoded, the sum $\\sum\\limits_{l = 1:l\\neq j}^m z_l^n= 1-z_j^n$, and we arrive at\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\partial C^n}{\\partial \\omega_{kj}} = - x^n_k \\left(z_j^n(1-\\hat{z}_j^n)-\\hat{z}_j^n\\sum\\limits_{l = 1. l\\neq j}^m z_l^n \\right) = - x^n_k \\left(z_j^n(1-\\hat{z}_j^n)-\\hat{z}_j^n (1 -  z_j^n) \\right) = - x^n_k \\left(z_j^n-\\hat{z}_j^n  \\right) .\n",
    "\\end{equation*}\n",
    "\n",
    "This result is achieved using the notation that $\\omega_{kj}$ is the weight connecting node $k$ upstream with node $j$ downstream. If the notation is switched, the desired formulation is achieved.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "FILL IN ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "FILL IN ANSWER\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "FILL IN ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
